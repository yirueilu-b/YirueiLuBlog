# 機器學習基石 ( 2 ) - Learning to Answer Yes/No

以判斷銀行要不要給顧客信用卡為例介紹一個可以做是非題的機器學習演算法。
機器學習做的事情就是有一個學習的演算法，我們叫做 $A$，這個演算法呢，會看兩件事情，一件事情是什麼？ 資料，我們叫做 $D$，另外一件事情是一個 $H$ Hypothesis Set， 這個這個假說的集合，那還要從這個 $H$ 裏面選一個 這個 $g$，這個 $g$ 會當作就是說銀行最後使用的公式來這個， 等於是學到的一個這個技能。

## Perceptron Hypothesis Set

介紹 $H$，以 Perceptron 為例。

### A Simple Hypothesis Set: the Perceptron

複習機器學習流程
![](https://i.imgur.com/sFjiMWs.png)

use $D$ and $H$ with $A$ to find $g$ that approxmate $f$

若每個使用者想成一個向量 $x$

$\text{for } x = (x_1,x_2,...,x_d)\text{, compute a weighted score and}$

$\begin{equation}
  \left\{
  \begin{array}{@{}ll@{}}
    approve, & \text{if}\ \sum_{i=1}^dw_ix_i > threshold \\
    deny, & \text{if}\ \sum_{i=1}^dw_ix_i < threshold
  \end{array}\right.
\end{equation}$

then

$\begin{equation}
  \left\{
  \begin{array}{@{}ll@{}}
    approve, & \text{if}\ \sum_{i=1}^dw_ix_i - threshold > 0 \\
    deny, & \text{if}\ \sum_{i=1}^dw_ix_i - threshold< 0
  \end{array}\right.
\end{equation}$

$Y: \{+1(good), -1(bad)\} \text{, 0 ignored-linear formula }h \in H are$

$h(x) = sign((\sum_{i=1}^dw_ix_i) - threshold)$

不同的 W，不同的門檻，造出不同的 h，h 就是 perceptron hypothesis

### Vector Form of Perceptron Hypothesis

把 $h$ 簡化表示

$h(x) = sign((\sum_{i=1}^dw_ix_i) + (-threshold) \cdot +1)$

$= sign(\sum_{i=0}^dw_ix_i)$

$= w^Tx$

### Perceptron in $R^2$

以 2 維空間來看 $h$ 的樣貌

![](https://i.imgur.com/KZ8ZDQr.png)
![](https://i.imgur.com/8yyO2HU.png)

每個 $h$ 對應到空間中的一個超平面

$\text{perceptrons} \Leftrightarrow \text{linear (binary) classifier}$

## Perceptron Learning Algorithm (PLA)

介紹 $A$ ( 如何藉由 $H$ 和 $D$ 學習到 $g\approx f$ )，以 PLA ( Perceptron Learning Algorithm ) 為例。

### Select $g$ from $H$

$H$ is all possible perceptrons, $g=?$

- want: $g \approx f$ ( hard when $f$ unknown )
- almost necessary: $g \approx f$ on $D$, ideally $g(x_n)=f(x_n)=y_n$
- difficult: size of $H$ is infinite
- idea: start from some $g_0$, and correct its mistake on $D$

如何找到 $g$ ? 先找到在資料上能夠和 $f$ 非常接近的 $g$ 最好一模一樣，也就是 $g(x_n)=f(x_n)=y_n$ 但是有無限多個 $h$ 因此先隨便找一個 $h$ ( $g_0$ )再藉由資料慢慢修正。

![](https://i.imgur.com/GtDdxzR.png)

以下使用 $w_0$ 來表示 $g_0$

### Perceptron Learning Algorithm

start from some $w_0$ ( say, 0 ), and correct its mistake on $D$

$\begin{equation}
  \text{For t = 0,1,2,...} \\
  \text{find a mistake of } w_t \text{ called } (x_{n(t)}, y_{n(t)})\\
  sign(w_t^Tx_{n(t)}) \ne y_{n(t)}\\
  \text{( try to ) correct the mistake by} \\
  w_{t+1} \leftarrow w_t + y_{n(t)}x_{n(t)} \\
  \text{until no more mistakes then return last } w \text{ ( called } w_{PLA} \text{ ) as }  g
\end{equation}$

當 $y=+1$ 時， $w$ 和 $x$ 角度太大，將 $w$ 拉回，反之將 $w$ 拉開
![](https://i.imgur.com/Lyk8z0z.png)

> A fault confessed is half redressed 知錯能改善莫大焉

### Practical Implementation of PLA

如何簡單判斷已經完全沒犯錯或如何找出還有沒有錯誤?

- naive cycle ( 1,...,N ) 一個一個檢查，若全部都看完沒錯則結束迴圈 ( cyclic PLA )
- precomputed random cycle 亂數決定順序，跳著檢查。

> Check the video 10:30 to see a simple demonstration for PLA

### Some Remaining Issue of PLA

是否一定會停止 ?
- Algorithmic: halt ( with no mistake ) ?
    - naive cyclic ?
    - random cyclic ?
    - other variant ?

找出的 $g$ 是否真的接近 $f$ ?
- learning: $g\approx f$ ?
    - on $D$, if halt, yes
    - outside $D$ ?
    - if not halting ?

## Guarantee of PLA

### Linear Separability

若 PLA 停止，資料 $D$ 必須存在一條不犯錯的線 $w$，這樣的資料稱為線性可分
- if PLA halts ( i.e. no more mistakes ), ( necessary condiion ) $D$ allows some $w$ to make no mistake
- call such $D$ linear separable

![](https://i.imgur.com/e5VDHxr.png)

assume that $D$ is linear separable, does PLA always halts ?

### PLA Fact: $w_t$ Gets More Aligned with $w_f$

**linear separable $D$ $\Leftrightarrow$ exists perfect $w_f$ such that $y_n=sign(w_t^Tx_n)$**

$w_f$ 這條線讓每個點都不會在線上也不會在錯誤的那邊

$w_f$ is perfect hence every $x_n$ correctly away from line:

$\min\limits_n y_nw_f^Tx_n > 0$

PLA 過程中選擇的點 ( 用來更新的點 $(x_{n(t)}, y_{n(t)})$ ) 也會滿足這個條件

$y_{n(t)}w_f^Tx_{n(t)} \geq \min\limits_n y_nw_f^Tx_n > 0$

有了上式就可以開始看 $w_t$ 與 $w_f$ 之間的關係

$w_f^T$ 與 $w_{t+1}$ 在不考慮長度時，兩向量接近的程度可以用 $w_f^T \cdot w_{t+1}$ 來表示，越大代表越接近 

$a \cdot b = \Vert a \Vert \Vert b \Vert \cos\theta$

$a \cdot b\ \uparrow\ \Rightarrow\ \theta\ \downarrow$

by updating with any $(x_{n(t)}, y_{n(t)})$

$w_f^Tw_{t+1} = w_f^T(w_t + y_{n(t)}x_{n(t))}) = w_f^Tw_t + y_{n(t)}w_f^Tx_{n(t))}$

$\geq w_f^Tw_t + \min\limits_n y_nw_f^Tx_n > w_f^Tw_t + 0$

$\Rightarrow w_f^Tw_{t+1} > w_f^Tw_t$

$w_t$ appears more aligned with $w_f$ after update

### PLA Fact: $w_t$ Does Not Grow Too Fast

**$w$ changed only when updating $\Leftrightarrow sign(w_t^Tx_{n(t)}) \neq y_{n(t)} \Leftrightarrow y_{n(t)}w_t^Tx_{n(t))} \leq 0$**

$\Vert w_{t+1}\Vert^2 = \Vert w_t + y_{n(t)}x_{n(t)} \Vert^2$

$= \Vert w_t \Vert^2 + 2y_{n(t)}x_{n(t)} + \Vert y_{n(t)}x_{n(t)} \Vert^2$

$\leq \Vert w_t \Vert^2 + 0 + \Vert y_{n(t)}x_{n(t)} \Vert^2$

$\leq \Vert w_t \Vert^2 + \max\limits_n\Vert x_{n} \Vert^2$

### Practice

Start from $w_0=0$, after T mistake corrections, $\frac{w_f^T}{\Vert w_f^T\Vert}\frac{w_T}{\Vert w_T\Vert} \geq \sqrt{T} \cdot constant$ , find the $constant$.

$\Vert w_T \Vert^2 \leq \color{red}{\Vert w_{T-1} \Vert^2 + \max\limits_n\Vert x_n \Vert^2}$

$\leq \color{red}{\Vert w_{0} \Vert^2 + T \max\limits_n \Vert x_n \Vert^2}$

$\leq T \max\limits_n \Vert x_n \Vert^2$

$\Rightarrow \Vert w_T \Vert \leq \sqrt T \max\limits_n \Vert x_n \Vert$

$w_f^Tw_T \geq \color{red}{w_f^Tw_{T-1} + \min\limits_ny_nw_f^Tx_n}$

$\color{red}{\geq w_f^Tw_{0} + T\min\limits_ny_nw_f^Tx_n}$

$\geq T\min\limits_ny_nw_f^Tx_n$

$\Rightarrow w_f^Tw_T \geq T\min\limits_ny_nw_f^Tx_n$

$\Rightarrow \frac{w_f^T}{\Vert w_f^t\Vert}\frac{w_T}{\Vert w_T\Vert} \geq \frac{T\min\limits_ny_nw_f^Tx_n}{\Vert w_f^T\Vert \sqrt T \max\limits_n \Vert x_n \Vert} = \sqrt{T} \frac{\min\limits_ny_nw_f^Tx_n}{\Vert w_f^T\Vert \max\limits_n \Vert x_n \Vert} \space_{\#}$

Define $R^2=\max\limits_n \Vert x_n \Vert^2$ and $\rho = \min\limits_ny_n\frac{w_f^T}{\Vert w_f^T \Vert}x_n$, 
We want to show that $T \leq U$. Express the upper bound U by $R^2$ and $\rho$

$\because \forall\ a, b \in \text{unit vector, } a \cdot b = \Vert a\Vert \Vert b\Vert \cos\theta = \cos\theta \leq 1$

$\therefore \sqrt{T} \frac{\min\limits_ny_nw_f^Tx_n}{\Vert w_f^T\Vert \max\limits_n \Vert x_n \Vert} \leq 1$

$\Rightarrow \sqrt{T} \leq \frac{\Vert w_f^T\Vert \max\limits_n \Vert x_n \Vert}{\min\limits_ny_nw_f^Tx_n} = \frac{\max\limits_n \Vert x_n \Vert}{\min\limits_ny_n\frac{w_f^T}{\Vert w_f^T \Vert}x_n} = \frac{R}{\rho}$

$\Rightarrow T \leq \frac{R^2}{\rho^2} \space_{\#}$

## Non-Separable Data

已證明在線性可分的資料上 PLA 會停止，看看若是線性不可分如何處理

### More about PLA

- Guarantee
    as long as $D$ is linear separable and PLA correct by mistake
    - $w_f \cdot w_t$ grows fast, $\Vert w_t\Vert$ grows slowly
    - PLA lines are more and more aligned with $w_f \Rightarrow$ halts

PLA 非常簡單、快速而且容易推廣到任意維度
- Pros
    - simple
    - fast
    - extendable for any dimension

不知道 PLA 是否會停，就算會停也不知道多久停
- Cons
    - assume $D$ is linear separable - property unknown in advance
    - not sure how long halting takes ( $\rho$ depends on $w_f$ )

### Learning with Noisy Data

資料可能有雜訊，就算實際的 $f$ 是一條線 ( $D$ 線性可分 )，也可能無法使用 PLA

![](https://i.imgur.com/6zVGs7v.png)

### Line with Noisy 

![](https://i.imgur.com/lpfr24o.png)

- assume little noise: $y_n = f(x_n)$ usually
- if so, $g \approx f on D \Leftrightarrow y_n = f(x_n)$ usually
- find a line with minimum mistakes:
    $w_g \leftarrow \arg\min\limits_{w}\sum_{n=1}^N[y_n \neq sign(w^Tx_n)]$
    **but it's NP-hard to solve**

### Pocket Algorithm

modify PLA ( black lines ) by keeping best weights in pocket

![](https://i.imgur.com/EZk4LXN.png)


###### tags: `Machine Learning Foundations`